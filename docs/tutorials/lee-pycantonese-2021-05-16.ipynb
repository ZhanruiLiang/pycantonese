{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "daily-attendance",
   "metadata": {},
   "source": [
    "# Accessing and Searching Cantonese Corpora in PyCantonese\n",
    "\n",
    "[Jackson Lee](https://jacksonllee.com/)\n",
    "\n",
    "May 16, 2021 (tutorial at the [School of Cantonese Studies](https://www.eduhk.hk/lml/scs2021/), the Education University of Hong Kong)\n",
    "\n",
    "Source of this Jupyter notebook: https://github.com/jacksonllee/pycantonese/blob/main/docs/tutorials/lee-pycantonese-2021-05-16.ipynb\n",
    "\n",
    "The easiest way to immediately play with this notebook is to log on to your Google account (Gmail, etc.) and [open this notebook in Google Colab](https://colab.research.google.com/github/jacksonllee/pycantonese/blob/main/docs/tutorials/lee-pycantonese-2021-05-16.ipynb). You'll have your own copy to run cells from, make changes, and save in your Google Drive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "computational-clinic",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This tutorial introduces [PyCantonese](https://pycantonese.org/), a Python library for Cantonese linguistics and natural language processing. In particular, this tutorial focuses on the following topics and provides hands-on exercises:\n",
    "\n",
    "1. Accessing existing Cantonese corpora\n",
    "2. Searching corpora programmatically"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mobile-exhibit",
   "metadata": {},
   "source": [
    "## Download and Install PyCantonese\n",
    "\n",
    "Just like all other open-source Python libraries, PyCantonese is readily available through the `pip install` terminal command. In this tutorial, we're pinning the version we want at the latest version v3.3.0 for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protective-response",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pycantonese==3.3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "needed-carry",
   "metadata": {},
   "source": [
    "## 1. Accessing Existing Cantonese Corpora\n",
    "\n",
    "For a corpus to be useful for PyCantonese, its source data files have to be publicly available in a machine-readable format, and the format has to be the [CHAT](https://talkbank.org/manuals/CHAT.pdf) format. This format was chosen for an important reason: A cluster of Cantonese corpora with publicly available data files are those on CHILDES and TalkBank, thanks to research on Cantonese language acquisition.\n",
    "\n",
    "PyCantonese is shipped with the [Hong Kong Cantonese Corpus](http://compling.hss.ntu.edu.sg/hkcancor/) (HKCanCor, CC BY license). We are going to use this corpus a lot in this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liberal-reflection",
   "metadata": {},
   "source": [
    "### Getting Started with HKCanCor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unknown-gravity",
   "metadata": {},
   "source": [
    "To give you a sense of what the HKCanCor data looks like, here are the first three utterances as they've been transformed to CHAT and incorporated into PyCantonese:\n",
    "\n",
    "```\n",
    "*XXA:\t喂 遲 啲 去 唔 去 旅行 啊 ?\n",
    "%mor:\te|wai3 a|ci4 u|di1 v|heoi3 d|m4 v|heoi3 vn|leoi5hang4 y|aa3 ?\n",
    "*XXA:\t你 老公 有冇 平 機票 啊 ?\n",
    "%mor:\tr|nei5 n|lou5gung1 v1|jau5mou5 a|peng4 n|gei1piu3 y|aa3 ?\n",
    "*XXB:\t平 機票 要 淡季 先 有得 平 𡃉 喎 .\n",
    "%mor:\ta|peng4 n|gei1piu3 vu|jiu3 an|daam6gwai3 d|sin1 vu|jau5dak1 a|peng4 y|gaa3 y|wo3 .\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expanded-extra",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycantonese\n",
    "\n",
    "corpus = pycantonese.hkcancor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approved-newman",
   "metadata": {},
   "source": [
    "What is HKCanCor's word count?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neutral-scanner",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = corpus.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adopted-welcome",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expanded-retrieval",
   "metadata": {},
   "source": [
    "Let's check out the first 10 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amazing-removal",
   "metadata": {},
   "outputs": [],
   "source": [
    "words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demographic-windows",
   "metadata": {},
   "source": [
    "Hmm, the output got cut off arbitrarily at 你, because `corpus.words()` has intentionally removed the *utterance-level* data structure in order to expose the corpus data as a flat (and very long) list of words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demanding-excerpt",
   "metadata": {},
   "source": [
    "### [ Exercise 1 ]\n",
    "\n",
    "`words` is a list of the words in HKCanCor. `words[:10]` gives you the first 10 words. If you want to see the first 20 words, what do you do? Type your code in the cell below and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "north-assurance",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "inclusive-hawaii",
   "metadata": {},
   "source": [
    "### Bringing Back the Utterances\n",
    "\n",
    "Let's look at the words again, but this time we use a list to wrap around each utterance's words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordinary-disaster",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_by_utterances = corpus.words(by_utterances=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colored-match",
   "metadata": {},
   "source": [
    "If we ask for a count by `len()` again, it's the number of utterances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correct-growth",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(words_by_utterances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sitting-difficulty",
   "metadata": {},
   "source": [
    "Here are the first two utterances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "front-endorsement",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_by_utterances[:2]  # a list of two lists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conditional-iceland",
   "metadata": {},
   "source": [
    "Let's check them out again but with a `for` loop instead. In actual work, a `for` loop is going to be much more useful for walking through corpus data and grabbing whatever is of interest. For now, we're simply `print`ing the two utterances as a start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooked-brass",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for utterance in words_by_utterances[:2]:\n",
    "    print(utterance)  # each utterance is a list of strings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finished-deadline",
   "metadata": {},
   "source": [
    "### Visualization Example: Utterance Lengths\n",
    "\n",
    "With access to the corpus source data, you can see just about anything you want. Besides examining contents of particular interest, you can zoom out for more general statistics and visualize them. Since we have the utterances in hand, here's something we can pull out real quick: what does the distribution of utterance lengths in HKCanCor look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animal-viking",
   "metadata": {},
   "outputs": [],
   "source": [
    "utterance_lengths = []\n",
    "\n",
    "for utterance in words_by_utterances:\n",
    "\n",
    "    # Utterances that are too long seem suspicious.\n",
    "    # Deciding where to cut for utterance boundaries is genearlly hard...\n",
    "    if len(utterance) > 50:\n",
    "        continue\n",
    "\n",
    "    utterance_lengths.append(len(utterance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "announced-herald",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Install packages for making plots.\n",
    "!pip install seaborn==0.11.1 matplotlib pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grave-heaven",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
    "sns.histplot(utterance_lengths, binwidth=2).set(xlabel=\"Utterance length\", ylabel=\"Count\", title=\"Count against utterance length (<= 50) in HKCanCor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offensive-fancy",
   "metadata": {},
   "source": [
    "### What About CHILDES / TalkBank?\n",
    "\n",
    "To illustrate how you may access a Cantonese CHILDES dataset, let's use the Cantonese monolingual [Lee-Wong-Leung corpus](https://childes.talkbank.org/access/Chinese/Cantonese/LeeWongLeung.html). All you need is the URL that points to the ZIP file containing the CHAT data files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informative-serum",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pycantonese.read_chat(\"https://childes.talkbank.org/data/Chinese/Cantonese/LeeWongLeung.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sexual-trademark",
   "metadata": {},
   "source": [
    "Before this previous cell, we'd been using the `corpus` variable name to represent HKCanCor. Now `corpus` has been repurposed to represent the Lee-Wong-Leung corpus instead. This is intentional - to imply that both instances of `corpus` would have the same functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overhead-bangkok",
   "metadata": {},
   "source": [
    "### [ Exercise 2 ]\n",
    "\n",
    "Write and run code to answer the following questions. The code you need can be found from above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "closed-fifth",
   "metadata": {},
   "source": [
    "How many words are there in the Lee-Wong-Leung corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-wayne",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "norman-federation",
   "metadata": {},
   "source": [
    "How many utterances are there in the Lee-Wong-Leung corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "after-substitute",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "premier-punch",
   "metadata": {},
   "source": [
    "Using a `for` loop, show the first ten utterances of the Lee-Wong-Leung corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "double-breakdown",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "mineral-samuel",
   "metadata": {},
   "source": [
    "### Visualization Example: Mean Length of Utterance (MLU)\n",
    "\n",
    "The mean length of utterance (MLU) is a standard measure of productivity in language acquisition research. Since PyCantonese uses the CHAT format from CHILDES and TalkBank, it can readily compute language development measure such as MLU.\n",
    "\n",
    "Let's use Timmy's data from the CHILDES Yip-Matthews Cantonese-English bilingual corpus. Between 2 and 3.5 years old, it appears that Timmy went from being Cantonese-dominant to English-dominant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logical-outreach",
   "metadata": {},
   "outputs": [],
   "source": [
    "timmy_cantonese = pycantonese.read_chat(\"https://childes.talkbank.org/data/Biling/YipMatthews.zip\", \"TimCan\")\n",
    "timmy_english = pycantonese.read_chat(\"https://childes.talkbank.org/data/Biling/YipMatthews.zip\", \"TimEng\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convertible-authorization",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\"MLU\": timmy_cantonese.mlu() + timmy_english.mlu(),\n",
    "     \"Language\": [\"Cantonese\"] * timmy_cantonese.n_files() + [\"English\"] * timmy_english.n_files(),\n",
    "     \"Age in months\": timmy_cantonese.ages(months=True) + timmy_english.ages(months=True)}\n",
    ")\n",
    "\n",
    "sns.lmplot(\n",
    "    x=\"Age in months\", y=\"MLU\", hue=\"Language\", data=df, markers=[\"o\", \"x\"], legend=True, legend_out=False, ci=None,\n",
    ").set(xlim=(22, 45), ylim=(0, 6), title=\"Timmy's MLU between 2 and 3.5 years old\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sticky-banking",
   "metadata": {},
   "source": [
    "## 2. Searching Corpora Programmatically\n",
    "\n",
    "When you need corpus data, there's a reason for that. This section is about a common scenario: You're looking for something specific from a corpus for your research. Your search criteria can be as simple or complex as desired, provided that you can programmatically tap into them in some way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prompt-witness",
   "metadata": {},
   "source": [
    "### Tokens as Words with Annotations\n",
    "\n",
    "HKCanCor is word-segmented as well as annotated for parts of speech and Jyutping romanization. While the `.words()` call gives us only the plain text words as Chinese/Cantonese characters, the annotations are accessible via `.tokens()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uniform-landscape",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pycantonese.hkcancor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "taken-hampton",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_by_utterances = corpus.tokens(by_utterances=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swedish-chase",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "We saw the very first utterance in HKCanCor as `['喂', '遲', '啲', '去', '唔', '去', '旅行', '啊', '?']` (as a list of plain text strings) through `.words()`. Let's check it out again with annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "difficult-showcase",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tokens_by_utterances[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instrumental-validation",
   "metadata": {},
   "source": [
    "This is a list of `Token` objects. Each `Token` object has a couple attributes:\n",
    "\n",
    "* `word`: the word form as Chinese/Cantonese characters\n",
    "* `pos`: part-of-speech tag (see the [HKCanCor documentation](http://compling.hss.ntu.edu.sg/hkcancor/) for the POS tagset)\n",
    "* `jyutping`: Jyutping romanization\n",
    "* (`mor` and `gra` are morphological information and grammatical relations, respectively. They are used in CHILDES and TalkBank datasets, but not in HKCanCor.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apart-bridge",
   "metadata": {},
   "source": [
    "The attributes from each token can be directly accessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming-tooth",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for token in tokens_by_utterances[0][:3]:\n",
    "    print(f\"word: {token.word}\")\n",
    "    print(f\"jyutping: {token.jyutping}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convertible-stanford",
   "metadata": {},
   "source": [
    "### [ Exercise 3 ]\n",
    "\n",
    "Using `tokens_by_utterances` we've just created, write code to generate the following output for the first utterance, where each line has a word followed by a space and then the word's part-of-speech tag. You should use a `for` loop.\n",
    "\n",
    "```\n",
    "喂 E\n",
    "遲 A\n",
    "啲 U\n",
    "去 V\n",
    "唔 D\n",
    "去 V\n",
    "旅行 VN\n",
    "啊 Y\n",
    "? ?\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "south-support",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "competent-decrease",
   "metadata": {},
   "source": [
    "### Searching by a Single Element\n",
    "\n",
    "Now that we know about the tokens, we're ready for programmatically searching a corpus.\n",
    "\n",
    "Let's say you're interested in gwai2 鬼, and that you'd like to pull out all occurrences of 鬼 from HKCanCor. The `.search()` call handles a search targeting a single word/token (\"search for all utterances with 鬼\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dental-counter",
   "metadata": {},
   "outputs": [],
   "source": [
    "gwai2 = corpus.search(character=\"鬼\", by_utterances=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "administrative-search",
   "metadata": {},
   "source": [
    "How many times does gwai2 鬼 occur in HKCanCor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "younger-infection",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gwai2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improved-addiction",
   "metadata": {},
   "source": [
    "What's the first utterance with 鬼?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incomplete-testing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gwai2[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "global-privacy",
   "metadata": {},
   "source": [
    "Show the first 20 utterances -- as the Chinese characters, don't bother with all the nitty-gritty details of the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turkish-utilization",
   "metadata": {},
   "outputs": [],
   "source": [
    "for utterance in gwai2[:20]:\n",
    "    print(\"\".join(token.word for token in utterance))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seasonal-interference",
   "metadata": {},
   "source": [
    "### [ Exercise 4 ]\n",
    "\n",
    "Suppose you're working on numerical classifiers, and that you want to find all of them from HKCanCor.\n",
    "\n",
    "1. What is the part-of-speech tag for classifiers. Check the [HKCanCor documentation](http://compling.hss.ntu.edu.sg/hkcancor/).\n",
    "2. We have the HKCanCor reader object `corpus` ready. How should we use the `.search()` call to say \"give me all utterances where a numerical classifier shows up\"? You may need to refer to the [PyCantonese documentation](https://pycantonese.org/searches.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fantastic-plumbing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "available-adoption",
   "metadata": {},
   "source": [
    "### More Complex Searches\n",
    "\n",
    "While the `.search()` method handles only one search criterion, you'd have to write your own custom code for more complex searches.\n",
    "\n",
    "Reminders:\n",
    "* To look for what you want, it's a matter of how you make use of the available corpus data and its annotations.\n",
    "* We've been using HKCanCor, but it's just one of the many possible corpora you could use.\n",
    "* After all, you drive your own research. You decide what you want the computer to do!\n",
    "\n",
    "Your use case may vary, but a basic approach looks like this:\n",
    "\n",
    "1. Create an empty container that's going to keep the search result.\n",
    "2. Loop through a corpus.\n",
    "3. Whenever you (= your code) see what's of interest, keep that in the container you've created.\n",
    "4. When the looping is over, the container should have what you're after."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "third-xerox",
   "metadata": {},
   "source": [
    "As an example, let's say you're interested in nominal syntax and semantics with respect to ge3 嘅. Now you want to grab all the utterances in HKCanCor where 嘅 appears in a nominal context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ready-revelation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install a package that makes it easy to get ngrams.\n",
    "!pip install nskipgrams==0.3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suited-guess",
   "metadata": {},
   "source": [
    "In our search below, what exactly are the search criteria?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smoking-filling",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nskipgrams\n",
    "\n",
    "# 1. Create an empty container that's going to keep the search result.\n",
    "result = []\n",
    "\n",
    "# 2. Loop through a corpus.\n",
    "for utterance in corpus.tokens(by_utterances=True):\n",
    "\n",
    "    # Set up a two-token sliding window and move it along the utterance.\n",
    "    for bigram in nskipgrams.ngrams_from_seq(utterance, 2):\n",
    "        token1, token2 = bigram\n",
    "\n",
    "        # 3. Whenever you (= your code) see what's of interest, keep that in the container you've created.\n",
    "        if token1.word == \"嘅\" and token2.pos.startswith(\"N\"):\n",
    "            result.append(utterance)\n",
    "\n",
    "            # Once you've saved this utterance,\n",
    "            # you don't need to check the rest of the utterance anymore.\n",
    "            break\n",
    "\n",
    "# 4. When the looping is over, the container should have what you're after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welsh-marshall",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stone-extent",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "result[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mathematical-inspiration",
   "metadata": {},
   "source": [
    "Questions: Did this search strategy miss cases of NP ellipsis? If we left out the search criterion that a noun must be present, would there be issues?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "democratic-sally",
   "metadata": {},
   "source": [
    "### [ Exercise 5 ]\n",
    "\n",
    "If you were to work on Cantonese verbal particles (aspect, etc.), what would you do to pull out all instances of verbal particles from HKCanCor?\n",
    "\n",
    "* What are verbal articles? Would you target a specific set of Chinese/Cantonese characters, or would you use the part-of-speech tags? A combination of both?\n",
    "* How much context would you like for each hit in the search result? A window of tokens around a found verbal particle? The utterance where it appears? A window of utterances?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hazardous-subcommittee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "limiting-majority",
   "metadata": {},
   "source": [
    "## There's More!\n",
    "\n",
    "PyCantonese, either on its own or combined with other tools, supports other functionality that we didn't have time to cover in this tutorial:\n",
    "\n",
    "* Jyutping parsing and conversion\n",
    "* Making use of speakers' demographics available from the metadata of CHAT-formatted corpus data\n",
    "* Stop words\n",
    "* Word segmentation\n",
    "* Part-of-speech tagging\n",
    "* Creating your own word-segmented, POS-tagged corpus from unanalyzed Cantonese text\n",
    "* Topic modeling\n",
    "* Sentiment analysis\n",
    "* and more?\n",
    "\n",
    "The goal of PyCantonese is to fill the Cantonese-specific gaps that other tools don't. Some of these items can be handled just by PyCantonese alone (hence [documented](https://pycantonese.org/)), others possible when combined with more programming and modeling chops - the sky is the limit! While there may be more tutorials, I hope you found this tutorial helpful."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
